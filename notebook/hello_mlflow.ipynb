{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f50b6fd-0841-47e1-9d9d-2e29e0ee7dcb",
   "metadata": {},
   "source": [
    "# Hello MLflow!\n",
    "\n",
    "In this notebook, we'll say hello with MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac0082-3b40-4681-875e-9a630f8e8d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecb7e4-839d-4783-a24d-0592785d5ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"MLflow version:\", mlflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e47e6-f3b9-4a85-a828-a3dbadd4e1b8",
   "metadata": {},
   "source": [
    "## Starting Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3da3cf-e5fa-4d30-bb75-48762506438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"first-experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212d4cd-0123-4b16-903c-089116ce624f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set experiment name\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b064b-79dd-4d16-a49b-081e37acf909",
   "metadata": {},
   "source": [
    "After running the above cell, there should be a folder named `mlruns` in the same directory with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21558402-5e17-4f33-ae2c-aab4762c8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b090509-1b32-494f-b83d-88a4a4ebb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.get_registry_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33283700-bad6-4239-a49a-0c10dff9ed7a",
   "metadata": {},
   "source": [
    "We will use the function below to run (dummy) experiment using MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c925adb-81e3-44f6-8533-bde34b1a880e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_experiment(experiment_name, run_name=\"\", output_dir=\"outputs\"):\n",
    "    rng = np.random.default_rng()\n",
    "    # set experiment name\n",
    "    mlflow.set_experiment(\n",
    "        experiment_name=experiment_name\n",
    "    )\n",
    "\n",
    "    # use context manager to start runs\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        print(\"Running experiment..\")\n",
    "        print(\"Run:\", run.info.run_uuid)\n",
    "\n",
    "        # set some tags\n",
    "        mlflow.set_tag(\"sample_name\", run_name)\n",
    "        mlflow.set_tag(\"run_file\", \"jupyter_notebook\")\n",
    "\n",
    "        # log some parameters\n",
    "        mlflow.log_param(\"sample_param\", rng.integers(0, 100))\n",
    "\n",
    "        # log some metrics\n",
    "        mlflow.log_metric(\"accuracy\", rng.random())\n",
    "        mlflow.log_metric(\"f1_score\", rng.random())\n",
    "\n",
    "        # to log artifacts, we need to have them in directory first\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir(parents=True)\n",
    "        model_filename = output_dir / \"model.joblib\"\n",
    "        with model_filename.open(\"w\") as f:\n",
    "            f.write(\"This is a model!\")\n",
    "\n",
    "        mlflow.log_artifact(str(output_dir), artifact_path=\"artifact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673fd99-e68d-44bf-bb5b-af38bae60dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_experiment(EXPERIMENT_NAME, run_name=\"run-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2e018-97f5-4ca3-a427-4cf0e2ababc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_experiment(EXPERIMENT_NAME, run_name=\"run-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63323ec9-772c-4700-a4b7-3ec01b2a07cf",
   "metadata": {},
   "source": [
    "## Inspecting Experiments and Runs\n",
    "\n",
    "After running some experiments, we can inspect them using different ways. MLflow provide several approach to do this:\n",
    "* using MLflow Tracking UI\n",
    "* querying programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f208d8-c7ab-4d5e-b343-8df7af19b5da",
   "metadata": {},
   "source": [
    "### Managing Experiments and Runs Programmatically\n",
    "\n",
    "To organize experiments and runs in it, typically, we use `mlflow.client` module or simply `MlflowClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bafad0-d6f8-405d-959e-6990243300c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9f950-447b-4ad0-bda3-a2b4954f01b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c7a8a-5e11-4203-b5c2-f6ab8d1c236d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55354e0c-574c-4d99-8058-ae4fd28d0128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5970b-db63-42ca-aad5-67173aabd1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3362d-af52-4ec9-9799-bf357cb42502",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d008d-783a-4d76-a2d0-1d45bde4544e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs[0].to_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657152b-4155-43d6-81a4-4fa34750e0fc",
   "metadata": {},
   "source": [
    "### Managing Experiments and Runs with Tracking UI\n",
    "\n",
    "If we store the metadata locally (inside `mlruns/`), then we could directly run `mlflow ui` command in the terminal to open up the tracking UI. The steps are:\n",
    "1. make sure you're already in the desired folder where there's `mlruns` folder\n",
    "2. launch terminal and run command: `mlflow ui`\n",
    "3. open browser and go to `localhost:5000`\n",
    "\n",
    "The default port is `5000`, but you can always change it using option `-p PORT`.\n",
    "\n",
    "![](../assets/img/mlflow_ui.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
